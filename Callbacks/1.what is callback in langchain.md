---

## 🔹 What are **callbacks** in LangChain?

In **LangChain**, a **callback** is a mechanism that lets you **listen to events** that happen while a chain, LLM, or tool is running.

Think of them as **“hooks”** into LangChain’s execution process.

* Whenever something happens (e.g., an LLM starts generating tokens, a chain step finishes, or a retriever fetches results), a **callback handler** can be triggered.
* This lets you monitor, log, or act on those events in **real time**.

---

## 🔹 Why callbacks are useful

* **Streaming** → print tokens as they are generated.
* **Logging** → save inputs/outputs/errors to a file or external service.
* **Debugging** → track what’s happening inside a chain.
* **Monitoring** → send updates to dashboards, UIs, or alerting systems.

---

## 🔹 How callbacks work in LangChain

There are three main pieces:

1. **BaseCallbackHandler**

   * The parent class for all handlers.
   * You subclass it to define what happens when certain events occur (like `on_llm_start`, `on_llm_new_token`, `on_chain_end`, etc.).

2. **Callback Handlers**

   * Concrete implementations (e.g., `StreamingStdOutCallbackHandler`, `FileCallbackHandler`, `UsageMetadataCallbackHandler`).
   * They define what to do when events fire.

   Example:

   * `StreamingStdOutCallbackHandler` → prints tokens as they come.
   * `FileCallbackHandler` → writes events to a file.

3. **Callback Manager**

   * Collects one or more handlers.
   * Ensures all registered handlers get notified when an event happens.

---

## 🔹 Example: Streaming with a callback

```python
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# Create an LLM with streaming + callback
llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0
)

# Call the LLM
llm.invoke("Write a haiku about the moon.")
```

Here’s what happens internally:

1. You call the model.
2. As the LLM generates tokens, the **callback manager** detects the `on_llm_new_token` event.
3. It triggers the `StreamingStdOutCallbackHandler`.
4. You see tokens print out live.

---

## 🔹 Analogy

Think of callbacks like **notifications on your phone**:

* Different apps (LLM, chain, retriever, tool) send out events.
* Handlers decide what to do with them (show alert, log to a file, print on screen).
* The callback manager is like the phone’s notification center, routing messages to the right place.

---

✅ So in short:
**Callbacks in LangChain = a system for listening to and reacting to events (like streaming tokens, logging progress, or handling errors).**

---
